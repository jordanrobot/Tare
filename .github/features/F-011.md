# F-011: Performance & Caching

**Status**: Planned  
**Priority**: P2 (Should)  
**Effort**: M (3-5 days)  
**Epic**: E-001 — Option A Hybrid Core  
**Dependencies**: F-009 (Composite Unit Construction), F-010 (Composite Unit Operator Support)

---

## Feature Overview

This feature optimizes performance of dimensional operations through strategic caching and microbenchmarks. The goal is to ensure that the dimensional algebra engine introduced in E-001 maintains or improves upon baseline performance while adding sophisticated functionality. The feature includes establishing a performance baseline, implementing targeted caches for hot paths, and documenting allocation budgets.

### User Story

**As a** Tare library user performing high-volume engineering calculations  
**I want** dimensional operations to execute with minimal performance overhead  
**So that** I can use the library in performance-critical applications without sacrificing throughput or increasing memory pressure

### Context

The dimensional algebra engine (F-002 through F-010) introduces several new computational paths:
- Unit resolution and normalization
- Dimension signature calculations
- Composite unit parsing
- Signature-to-preferred-unit mapping
- Rational arithmetic for exact conversions

While these operations are individually fast, repeated executions in hot paths (operators, parsing, formatting) can accumulate overhead. F-011 ensures that:
1. Performance is measured and understood (no regressions)
2. Common operations are cached where beneficial
3. Allocation budgets are reasonable and documented
4. Benchmarks provide ongoing performance validation

---

## Problem Statement

### Current State (Post-F-010)

After implementing F-002 through F-010, the Tare library has:
- ✓ Full dimensional algebra support
- ✓ Composite unit construction and formatting
- ✓ Operator support for all unit types
- ✓ Correct behavior with proper test coverage
- ? **Performance characteristics unknown**
- ? **No caching strategy**
- ? **No allocation budget documentation**
- ? **No benchmark infrastructure**

### Performance Concerns (Hypothetical, Requires Measurement)

Several hot paths may benefit from optimization:

1. **Unit Resolution** (`UnitResolver.Resolve`):
   - Called on every operator invocation
   - Normalizes units, computes factors, resolves signatures
   - Currently no caching of resolved units

2. **Composite Parsing** (`CompositeParser.TryParse`):
   - Regex-based parsing with multiple allocations
   - Called for every composite unit construction and formatting
   - Results not cached

3. **Signature Mapping** (`KnownSignatureMap.TryGetPreferredUnit`):
   - Dictionary lookup is fast, but called repeatedly
   - Same signatures looked up multiple times in tight loops

4. **Normalization** (`UnitResolver.Normalize`):
   - Alias resolution via `UnitDefinitions.Parse`
   - Token creation and dictionary lookups
   - No caching of normalized tokens

### Desired State (After F-011)

- ✅ **Baseline established**: Benchmarks for key operations (operators, parsing, formatting)
- ✅ **Performance documented**: Clear understanding of operation costs
- ✅ **Strategic caching**: Fast paths benefit from memoization where appropriate
- ✅ **No regressions**: Post-cache performance equals or exceeds baseline
- ✅ **Allocation budget**: Memory overhead documented and reasonable
- ✅ **Ongoing validation**: Benchmarks runnable for future changes

---

## Technical Design

### Performance Analysis Strategy

#### Step 1: Establish Baseline (No Cache)

Create benchmarks measuring current performance across:

1. **Operator benchmarks**:
   - Catalog unit operations: `meter * meter`, `pound / ounce`
   - Composite unit operations: `"Nm" * "m"`, `"lbf*in" / "lbf"`
   - Mixed operations: catalog × composite

2. **Parsing benchmarks**:
   - Catalog units: `Quantity.Parse("10 m")`
   - Simple composites: `Quantity.Parse("50 Nm")`
   - Complex composites: `Quantity.Parse("100 kg*m^2/s^2")`

3. **Formatting benchmarks**:
   - To catalog units: `q.Format("in")`
   - To simple composites: `q.Format("Nm")`
   - To complex composites: `q.Format("lbf*in^2/s^3")`

4. **Resolution benchmarks** (internal):
   - `UnitResolver.Resolve("m")`
   - `UnitResolver.Resolve("Nm")`
   - `CompositeParser.TryParse("kg*m/s^2")`

**Baseline Metrics** (to be established):
- Throughput (operations/second)
- Latency (nanoseconds per operation)
- Allocations (bytes per operation)
- Gen 0/1/2 collection counts

#### Step 2: Identify Hot Paths

Profile benchmarks to identify:
- Most frequently called methods
- Highest allocation sources
- Operations with repeated identical inputs
- Critical paths for optimization

**Expected Hot Paths** (hypothesis, requires validation):
1. `UnitResolver.Resolve` (called 2x per operator invocation)
2. `CompositeParser.TryParse` (called for every composite)
3. `UnitToken` construction (frequent allocations)
4. `UnitDefinitions.Parse` (alias lookup)

#### Step 3: Design Caching Strategy

**Caching Principles**:
- Cache only when measurement shows benefit (don't over-optimize)
- Thread-safe caches (library may be used concurrently)
- Bounded cache sizes (prevent unbounded growth)
- Simple eviction policy (FIFO or size-based)
- Minimal overhead (cache check must be cheaper than computation)

**Candidate Caches**:

1. **Resolved Unit Cache** (`UnitResolver`):
   ```csharp
   // Cache: string unit → NormalizedUnit
   private readonly ConcurrentDictionary<string, NormalizedUnit> _resolvedCache;
   // Tunable: Increase if cache hit rate < 70% in production workloads
   private const int MaxCacheEntries = 128; // Conservative starting size
   ```
   - **Key**: Unit string (e.g., "m", "Nm", "lbf*in")
   - **Value**: `NormalizedUnit` (token, factor, signature, type)
   - **Benefit**: Eliminates repeated parsing/normalization
   - **Cost**: Memory (128 entries × ~64 bytes ≈ 8KB)

2. **Composite Parse Cache** (`CompositeParser`):
   ```csharp
   // Cache: string composite → (DimensionSignature, decimal factor)
   private readonly ConcurrentDictionary<string, (DimensionSignature, decimal)> _parseCache;
   // Tunable: Increase if composite operations are frequent and hit rate < 70%
   private const int MaxParseCache = 128; // Conservative starting size
   ```
   - **Key**: Composite string
   - **Value**: Parsed signature and factor
   - **Benefit**: Eliminates regex parsing overhead
   - **Cost**: Memory (128 entries × ~48 bytes ≈ 6KB)

3. **Signature Formatting Cache** (`CompositeFormatter`):
   ```csharp
   // Cache: DimensionSignature → formatted string
   private readonly ConcurrentDictionary<DimensionSignature, string> _formatCache;
   // Tunable: Increase if formatting operations are frequent and hit rate < 70%
   private const int MaxFormatCache = 128; // Conservative starting size
   ```
   - **Key**: `DimensionSignature`
   - **Value**: Formatted composite string (e.g., "kg*m/s^2")
   - **Benefit**: Eliminates string building overhead
   - **Cost**: Memory (128 entries × ~40 bytes ≈ 5KB)

**Total Estimated Cache Memory**: ~19KB (conservative overhead)

#### Step 4: Implementation Approach

**Option A: ConcurrentDictionary with Size Limit (Recommended)**

Use `ConcurrentDictionary<TKey, TValue>` with manual size enforcement:

```csharp
private NormalizedUnit ResolveWithCache(string unit)
{
    // Try cache first
    if (_resolvedCache.TryGetValue(unit, out var cached))
    {
        return cached;
    }
    
    // Compute if not cached
    var resolved = ResolveCore(unit); // Original logic
    
    // Add to cache if not at capacity
    // Note: MaxCacheEntries is tunable - increase if hit rate < 70% in production
    if (_resolvedCache.Count < MaxCacheEntries)
    {
        _resolvedCache.TryAdd(unit, resolved);
    }
    // Simple size-based eviction: stop growing when at capacity
    // Alternative: Remove random/oldest entry if more sophisticated eviction needed
    
    return resolved;
}
```

**Pros**:
- Built-in thread-safety (`ConcurrentDictionary`)
- Simple implementation
- No external dependencies
- Good performance for read-heavy workloads

**Cons**:
- No automatic eviction (manual size check required)
- No LRU or sophisticated eviction policy

**Option B: MemoryCache (ASP.NET Core)**

Use `Microsoft.Extensions.Caching.Memory.MemoryCache`:

```csharp
private readonly MemoryCache _cache = new MemoryCache(new MemoryCacheOptions
{
    SizeLimit = 512
});

private NormalizedUnit ResolveWithCache(string unit)
{
    return _cache.GetOrCreate(unit, entry =>
    {
        entry.Size = 1;
        entry.SetSlidingExpiration(TimeSpan.FromMinutes(10));
        return ResolveCore(unit);
    });
}
```

**Pros**:
- Built-in eviction policies (LRU, time-based)
- Automatic size management
- Well-tested framework component

**Cons**:
- **Adds external dependency** (`Microsoft.Extensions.Caching.Memory`)
- Violates epic requirement: "No mandatory external dependencies added"
- Overkill for simple use case

**Recommendation**: **Option A (ConcurrentDictionary)** - maintains zero-dependency goal while providing thread-safe caching.

#### Step 5: Validate Performance Improvements

Re-run benchmarks with caching enabled:

1. **Compare baseline vs. cached**:
   - Throughput improvement (target: ≥10% for cached operations)
   - Latency reduction (target: ≥20% for repeated operations)
   - Allocation reduction (target: ≥30% for hot paths)

2. **Measure cache effectiveness**:
   - Hit rate (target: ≥70% for repeated operations)
   - Miss rate
   - Cache size utilization

3. **Test edge cases**:
   - Cold cache (first access)
   - Warm cache (repeated access)
   - Cache saturation (> MaxEntries)
   - Concurrent access (multiple threads)

4. **Allocation profiling**:
   - Gen 0 collections (should decrease)
   - Gen 1/2 collections (should remain stable)
   - Total allocated bytes (document increase from caches)

---

## Implementation Plan

### Phase 1: Benchmark Infrastructure (1.5 days)

1. **Add BenchmarkDotNet**:
   - Create new project: `benchmarks/Tare.Benchmarks.csproj`
   - Add package: `BenchmarkDotNet` (standard industry tool)
   - Configure for both `netstandard2.0` and `net7.0` targets

2. **Create benchmark classes**:
   - `OperatorBenchmarks.cs`: Test `*`, `/`, `+`, `-` operators
   - `ParsingBenchmarks.cs`: Test `Parse` and constructor
   - `FormattingBenchmarks.cs`: Test `Format` method
   - `InternalBenchmarks.cs`: Test `UnitResolver`, `CompositeParser` (internal)

3. **Establish baseline**:
   - Run benchmarks on current codebase (F-010 completed)
   - Document results in `docs/performance-baseline.md`
   - Identify top 3-5 optimization opportunities

**Deliverables**:
- Benchmark project runnable via `dotnet run -c Release`
- Baseline performance data documented
- Hot path analysis completed

### Phase 2: Implement Caching (2 days)

1. **UnitResolver cache**:
   - Add `ConcurrentDictionary<string, NormalizedUnit>` field
   - Modify `Resolve` method to check cache first
   - Implement size-bounded cache with simple eviction
   - Add internal property for cache hit rate (diagnostics)

2. **CompositeParser cache**:
   - Add `ConcurrentDictionary<string, (DimensionSignature, decimal)>` field
   - Modify `TryParse` to check cache before regex parsing
   - Implement bounded cache (smaller than UnitResolver)

3. **CompositeFormatter cache** (optional, if profiling shows benefit):
   - Add `ConcurrentDictionary<DimensionSignature, string>` field
   - Cache formatted signature strings
   - Only implement if benchmarks show significant benefit

4. **Thread-safety validation**:
   - Add concurrent benchmark to verify thread-safety
   - Test cache under contention (10+ threads)
   - Ensure no race conditions or deadlocks

**Deliverables**:
- Caching implemented in `UnitResolver` and `CompositeParser`
- Thread-safe implementation verified
- Code follows existing style and conventions

### Phase 3: Performance Validation (1 day)

1. **Re-run benchmarks**:
   - Execute all benchmark suites with caching enabled
   - Compare baseline vs. cached results
   - Generate performance reports

2. **Analyze results**:
   - Calculate improvement percentages
   - Identify any regressions (investigate if found)
   - Document cache hit rates

3. **Allocation analysis**:
   - Measure total heap allocations
   - Document cache memory overhead
   - Ensure allocations are reasonable (<1MB for typical usage)

4. **Edge case testing**:
   - Test with cache disabled (ensure fallback works)
   - Test cache saturation scenarios
   - Test concurrent access patterns

**Deliverables**:
- Performance comparison report
- Allocation budget documentation
- No regressions vs. baseline

### Phase 4: Documentation (0.5 days)

1. **Update E-001 epic**:
   - Mark F-011 as completed
   - Document performance characteristics

2. **Create performance documentation**:
   - `docs/performance.md`: Guidance on performance-critical usage
   - Document cache behavior and memory overhead
   - Provide benchmark results and interpretation

3. **Update CHANGELOG.md**:
   - Add entry for F-011 with performance improvements
   - Note: Internal optimization, no API changes

4. **Add benchmark README**:
   - `benchmarks/README.md`: How to run benchmarks
   - Interpretation guide for results
   - How to add new benchmarks

**Deliverables**:
- Documentation updated
- CHANGELOG entry added
- Benchmarks documented for future use

---

## Test Plan

### Benchmark Test Matrix

| Category | Benchmark | Scenario | Expected Result |
|----------|-----------|----------|-----------------|
| **Operators** | Multiply_CatalogUnits | `meter * meter` | Baseline: <100ns/op |
| | Multiply_CompositeUnits | `"Nm" * "m"` | Cached: >50% faster |
| | Divide_CatalogUnits | `pound / ounce` | Baseline: <100ns/op |
| | Divide_CompositeUnits | `"lbf*in" / "lbf"` | Cached: >50% faster |
| **Parsing** | Parse_CatalogUnit | `Parse("10 m")` | Baseline: <200ns/op |
| | Parse_SimpleComposite | `Parse("50 Nm")` | Cached: >40% faster |
| | Parse_ComplexComposite | `Parse("100 kg*m^2/s^2")` | Cached: >60% faster |
| **Formatting** | Format_ToCatalogUnit | `q.Format("in")` | Baseline: <150ns/op |
| | Format_ToComposite | `q.Format("Nm")` | Cached: >30% faster |
| **Internal** | UnitResolver_Resolve | `Resolve("m")` | Cache hit: <20ns/op |
| | CompositeParser_TryParse | `TryParse("kg*m/s^2")` | Cache hit: <30ns/op |
| **Concurrency** | ParallelOperations | 10 threads × 1000 ops | No deadlocks, consistent perf |

**Note**: Actual performance targets will be established after baseline measurement. Percentages are hypothetical goals.

### Functional Tests

No new functional tests required (caching is internal optimization). However:

1. **Verify all existing tests pass** (387 tests):
   - Unit tests (catalog operations)
   - Integration tests (composite operations)
   - Edge case tests

2. **Add cache diagnostics tests** (internal):
   - `UnitResolver_Cache_HitsOnRepeatedAccess()`
   - `UnitResolver_Cache_RespectsSizeLimit()`
   - `CompositeParser_Cache_ThreadSafe()`

3. **Add benchmark validation**:
   - Ensure benchmarks compile and run without errors
   - Verify benchmark results are reproducible

---

## Acceptance Criteria

### Performance Requirements

✅ **AC-1**: Baseline performance documented for key operations (operators, parsing, formatting)  
✅ **AC-2**: Caching implemented in `UnitResolver` and `CompositeParser`  
✅ **AC-3**: Cached operations show measurable improvement (≥20% for repeated operations)  
✅ **AC-4**: No performance regressions vs. baseline for any operation  
✅ **AC-5**: Thread-safe cache implementation verified under concurrent load  
✅ **AC-6**: Allocation budget documented (total cache overhead <1MB)  
✅ **AC-7**: Cache hit rates measured and documented (target ≥70% for repeated operations)

### Quality Requirements

✅ **AC-8**: All existing tests pass (387 tests, no regressions)  
✅ **AC-9**: Benchmark infrastructure operational and documented  
✅ **AC-10**: Benchmarks runnable via `dotnet run -c Release` in benchmarks project  
✅ **AC-11**: No external dependencies added (zero-dependency goal maintained)  
✅ **AC-12**: Code follows existing style and conventions  
✅ **AC-13**: Internal cache diagnostics available for monitoring

### Documentation Requirements

✅ **AC-14**: Performance baseline documented in `docs/performance-baseline.md`  
✅ **AC-15**: Performance improvements documented in `docs/performance.md`  
✅ **AC-16**: Benchmark README created with usage instructions  
✅ **AC-17**: CHANGELOG updated with F-011 entry  
✅ **AC-18**: E-001 epic updated (F-011 marked complete)

---

## Risks and Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Cache overhead exceeds benefit | Low | Medium | Measure first, implement only if beneficial; make cache size tunable |
| Thread-safety issues | Low | High | Use `ConcurrentDictionary`; add concurrent benchmark tests |
| Memory pressure from caches | Low | Medium | Bounded cache sizes (128 each); document overhead; tunable constants |
| Benchmark infrastructure adds complexity | Medium | Low | Keep benchmarks separate; optional to run; document clearly |
| Performance varies by workload | Medium | Low | Test multiple scenarios; document cache effectiveness per workload |
| BenchmarkDotNet adds dependency | Low | Low | Only dev dependency (not runtime); standard tool in .NET ecosystem |

---

## Success Metrics

1. **Performance**: Cached operations ≥20% faster for repeated inputs
2. **No Regressions**: All operations perform at or above baseline
3. **Test Coverage**: All 387 existing tests pass
4. **Allocation Budget**: Total cache overhead <1MB documented
5. **Cache Effectiveness**: Hit rate ≥70% for typical workloads
6. **Documentation**: Complete performance guide and benchmark README

---

## Dependencies

### Upstream Dependencies (Required)

- **F-009**: Composite Unit Construction ✓ (Provides `CompositeParser`)
- **F-010**: Composite Unit Operator Support (Provides operator hot paths)

### Downstream Consumers (Will Benefit)

- **F-012**: Error Handling & Diagnostics (can report cache stats)
- **F-013**: API Helpers (may expose cache diagnostics if useful)
- All user code (transparent performance improvement)

---

## Cache Design Considerations

### Why ConcurrentDictionary over MemoryCache?

**Decision**: Use `ConcurrentDictionary<TKey, TValue>` for all caches.

**Rationale**:
1. **Zero Dependencies**: Maintains epic requirement (no external runtime dependencies)
2. **Simplicity**: Straightforward implementation, easy to understand
3. **Performance**: Excellent for read-heavy workloads (our use case)
4. **Thread-Safety**: Built-in, no additional locking required
5. **Predictability**: Bounded size with simple eviction is predictable

**Trade-offs**:
- No automatic eviction (must check size manually)
- No LRU or time-based expiration (not needed for our use case)
- Slightly more code than MemoryCache

**Eviction Strategy**:
- Check cache size before adding new entries
- If at capacity, don't add (simple "stop growing" policy)
- Alternative: Remove random entry (slightly more complex)
- LRU not required (unit strings don't have temporal locality)

### Cache Size Tuning

**Initial Conservative Sizes** (Per Maintainer Guidance):
- `UnitResolver`: 128 entries
- `CompositeParser`: 128 entries
- `CompositeFormatter`: 128 entries

**Rationale for Starting Conservative**:
- Start small to establish baseline and measure actual cache effectiveness
- 128 entries should cover typical engineering applications
- Memory overhead minimal (~19KB total)
- Easy to increase later if benchmarks show need (hit rate < 70%)
- Constants are clearly documented and easy to adjust

**Tuning Guidance for Maintainers**:
- Cache size constants are defined at the top of each class with clear comments
- If cache hit rate < 70%, consider doubling the size (128 → 256 → 512)
- If memory overhead becomes concern (>1MB), reduce or disable caching
- Monitor via internal diagnostics (cache hit rate property)
- Example tuning path based on measurements:
  - Start: 128 entries per cache (~19KB total)
  - If needed: 256 entries per cache (~38KB total)
  - If needed: 512/256/128 entries (~50KB total)
  - Maximum reasonable: 1024 entries per cache (~150KB total)

### Thread-Safety Guarantees

`ConcurrentDictionary` provides:
- **Safe Add**: `TryAdd` is atomic
- **Safe Get**: `TryGetValue` is atomic
- **Safe Check**: `Count` is consistent

Our implementation:
- No locks required
- No race conditions possible
- Cache may temporarily exceed size limit (rare, acceptable)
- Reads never block writes (high concurrency)

---

## Benchmark Infrastructure

### Project Structure

```
benchmarks/
├── Tare.Benchmarks.csproj        # Benchmark project
├── README.md                      # How to run benchmarks
├── OperatorBenchmarks.cs          # Operator performance tests
├── ParsingBenchmarks.cs           # Parsing performance tests
├── FormattingBenchmarks.cs        # Formatting performance tests
├── InternalBenchmarks.cs          # Internal API performance tests
└── BenchmarkConfig.cs             # Shared benchmark configuration
```

### Running Benchmarks

```bash
# Run all benchmarks
cd benchmarks
dotnet run -c Release

# Run specific benchmark
dotnet run -c Release --filter "*Operator*"

# Generate report
dotnet run -c Release --exporters html json
```

### Benchmark Configuration

```csharp
[Config(typeof(BenchmarkConfig))]
public class OperatorBenchmarks
{
    [Benchmark(Baseline = true)]
    public Quantity Multiply_CatalogUnits()
    {
        return _meter * _meter;
    }
    
    [Benchmark]
    public Quantity Multiply_CompositeUnits()
    {
        return _newtonMeter * _meter;
    }
}
```

**BenchmarkDotNet Features Used**:
- Warmup iterations (eliminate JIT overhead)
- Multiple iterations (statistical significance)
- Memory diagnostics (allocations, Gen 0/1/2)
- Baseline comparison (measure improvement)
- Export to HTML/JSON (reporting)

---

## Performance Baseline (To Be Established)

**Note**: These are hypothetical targets. Actual baseline will be measured in Phase 1.

| Operation | Expected Baseline | Target (Cached) | Improvement |
|-----------|-------------------|-----------------|-------------|
| Catalog Multiply | 80ns | 80ns | 0% (no cache) |
| Composite Multiply | 500ns | 200ns | 60% |
| Catalog Parse | 150ns | 150ns | 0% (no cache) |
| Composite Parse | 800ns | 250ns | 69% |
| Catalog Format | 120ns | 120ns | 0% (no cache) |
| Composite Format | 600ns | 300ns | 50% |

**Memory Allocations**:
- Catalog operations: 0 allocations (struct operations)
- Composite operations (baseline): ~5-10 allocations per operation
- Composite operations (cached): ~0-1 allocations per operation (cache hit)

---

## Open Questions

### Resolved (Per Maintainer Guidance)

1. **Cache size tuning**: Should cache sizes be configurable via API?
   - **Decision**: No API configuration. Cache sizes are internal constants.
   - **Implementation**: Start with 128 entries for all caches. Use clear constant names with comments to make future adjustments easy for maintainers. Include tuning guidance in documentation.

2. **BenchmarkDotNet dependency**: Acceptable as dev-only dependency?
   - **Decision**: Yes, dev-only dependency is acceptable.
   - **Implementation**: Add as `<PackageReference>` in benchmarks project only, not in main library.

3. **Benchmark CI jobs**: Add benchmark CI job?
   - **Decision**: Not in F-011. Keep benchmark project for future manual use.
   - **Implementation**: Create runnable benchmark project with clear README. CI integration can be future work if regressions occur.

### Open (For Implementation Phase)

4. **Cache warming**: Should we pre-populate caches with common units?
   - **Recommendation**: No. Let caches warm naturally. Pre-population adds complexity.
   - **Rationale**: Simple is better; common units will be cached quickly during normal use.

5. **Cache diagnostics**: Expose cache hit rates via public API?
   - **Recommendation**: No for F-011. Consider in F-013 (API Helpers) if useful.
   - **Rationale**: Internal diagnostics sufficient for initial implementation; can expose later if needed.

6. **Benchmark scope**: Include comparison with other libraries?
   - **Recommendation**: No. Focus on internal baseline and improvements.
   - **Rationale**: Goal is to ensure no regressions and document improvements, not competitive analysis.

---

## References

### Internal References

- **E-001**: Epic — Option A Hybrid Core
- **F-009**: Composite Unit Construction (provides `CompositeParser`)
- **F-010**: Composite Unit Operator Support (provides operator hot paths)
- **F-012**: Error Handling & Diagnostics (consumer)

### External References

- **BenchmarkDotNet Documentation**: https://benchmarkdotnet.org/
- **ConcurrentDictionary Performance**: https://learn.microsoft.com/en-us/dotnet/api/system.collections.concurrent.concurrentdictionary-2
- **.NET Performance Best Practices**: https://learn.microsoft.com/en-us/dotnet/framework/performance/performance-best-practices

### Performance Resources

- **Profiling Tools**: dotTrace, PerfView, Visual Studio Profiler
- **Memory Profiling**: dotMemory, ANTS Memory Profiler
- **Allocation Analysis**: `dotnet-counters`, `dotnet-trace`

---

## Notes

- This feature is purely internal optimization (no public API changes)
- Zero external runtime dependencies maintained (BenchmarkDotNet is dev-only)
- Caching is transparent to users (no behavior changes)
- Benchmarks provide ongoing performance validation
- Conservative cache sizes (128 entries each) ensure minimal memory overhead (~19KB)
- Cache size constants clearly documented for easy maintainer adjustment
- Thread-safe implementation suitable for concurrent applications
- Start small, measure, then increase cache sizes if needed based on hit rates

---

## Revision History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2025-11-10 | Copilot Agent | Initial planning document created per user request |
| 1.1 | 2025-11-11 | Copilot Agent | Updated cache sizes to 128 (conservative start), resolved open questions per maintainer guidance, added tuning guidance |
